{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove the combined mask at first \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob \n",
    "combined_scan = glob.glob('New/s*/*/combined*')\n",
    "print(len(combined_scan ))\n",
    "# remove these files\n",
    "for file in combined_scan:\n",
    "    os.remove(file)\n",
    "combined_scan = glob.glob('New/s*/*/combined*')\n",
    "print(len(combined_scan ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_scan = glob.glob('New/s0001/segmentations/*')\n",
    "len(combined_scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate labels found.\n",
      "Maximum label value: 117\n",
      "Number of unique labels: 117\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel('label_names.xlsx')\n",
    "\n",
    "# Extract the multiplied_labels column\n",
    "multiplied_labels = df['multiplied_labels'].tolist()\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = [label for label in multiplied_labels if multiplied_labels.count(label) > 1]\n",
    "if duplicates:\n",
    "    print(\"Duplicate labels found:\", set(duplicates))\n",
    "else:\n",
    "    print(\"No duplicate labels found.\")\n",
    "\n",
    "# Check the maximum label value\n",
    "max_label = max(multiplied_labels)\n",
    "print(\"Maximum label value:\", max_label)\n",
    "\n",
    "# Check the number of unique labels\n",
    "unique_labels = set(multiplied_labels)\n",
    "print(\"Number of unique labels:\", len(unique_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adrenal_gland_left.nii.gz 117\n",
      "1 117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders:   0%|          | 0/1228 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders: 100%|██████████| 1228/1228 [7:11:35<00:00, 21.09s/it]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import os \n",
    "\n",
    "# Load label information from Excel\n",
    "df = pd.read_excel('label_names.xlsx')\n",
    "label_names = df['label_names'].tolist()\n",
    "multiplied_labels = df['multiplied_labels'].tolist()\n",
    "print(label_names[0], len(label_names))\n",
    "print(multiplied_labels[0], len(multiplied_labels))\n",
    "\n",
    "# Get list of folders containing masks\n",
    "folders = glob.glob('New/s*/se*')\n",
    "folders.sort()\n",
    "\n",
    "for folder_num, folder in enumerate(tqdm.tqdm(folders, desc=\"Processing folders\")):\n",
    "    # Load one mask to get the shape and affine information\n",
    "    initial_file = os.path.join(folder, label_names[0])\n",
    "    initial_mask_nii = nib.load(initial_file)\n",
    "    mask_shape = initial_mask_nii.get_fdata().shape\n",
    "    affine = initial_mask_nii.affine\n",
    "\n",
    "    # Initialize the combined mask with zeros\n",
    "    combined_mask = np.zeros(mask_shape)\n",
    "    non_empty_label_count = 0\n",
    "    num_masks = []\n",
    "\n",
    "    for label_num, (label_name, label_value) in enumerate(zip(label_names, multiplied_labels)):\n",
    "        file_path = os.path.join(folder, label_name)\n",
    "        comb_file_path = os.path.join(folder, f'combined_mask.nii.gz')\n",
    "\n",
    "        file = nib.load(file_path)\n",
    "        mask = file.get_fdata()\n",
    "        combined_scan = nib.load(comb_file_path).get_fdata()\n",
    "\n",
    "        \n",
    "        \n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        else:\n",
    "            non_empty_label_count += 1\n",
    "            num_masks.append(label_value)\n",
    "    \n",
    "    #print(num_masks)\n",
    "    #print(np.unique(combined_scan))\n",
    "    # get the unique values in between combined mask and num_masks\n",
    "    unique_values = np.unique(combined_scan)\n",
    "    unique_values = np.setdiff1d(unique_values, num_masks)\n",
    "    #print()\n",
    "    \n",
    "    if len(unique_values) > 1:\n",
    "        print(folder, len(np.unique(combined_scan)), unique_values)\n",
    "    #print(non_empty_label_count, len(np.unique(combined_scan)), unique_values)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glob.glob('dataset_nii/val/labels/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1171"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171/1171 [08:58<00:00,  2.17it/s]\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "file = glob.glob('dataset_nii/*/labels/*')\n",
    "for f in tqdm(file):\n",
    "    mask = nib.load(f).get_fdata()\n",
    "    number = np.unique(mask)\n",
    "    if 117 in number:\n",
    "        print(f, number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1082\n",
      "57\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "split\n",
       "train    1082\n",
       "test       89\n",
       "val        57\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('meta.xlsx')\n",
    "df1 = df[df['split']=='train']\n",
    "print(len(df1))\n",
    "df2 = df[df['split']=='val']\n",
    "print(len(df2))\n",
    "len(df1) + len(df2)\n",
    "df['split'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Devide the datset based on split column value 'train' or 'test'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['image_id', 'age', 'gender', 'institute', 'study_type', 'split',\n",
      "       'manufacturer', 'scanner_model', 'kvp', 'pathology',\n",
      "       'pathology_location'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1082/1082 [00:13<00:00, 82.04it/s] \n"
     ]
    }
   ],
   "source": [
    "# read the mketa.xlsx file \n",
    "import pandas as pd\n",
    "df = pd.read_excel('meta.xlsx')\n",
    "# select only train values in split column\n",
    "df = df[df['split']=='train']\n",
    "print(df.columns)\n",
    "# make directories for train and test\n",
    "import os\n",
    "os.makedirs('dataset_nii/train/images', exist_ok=True)\n",
    "os.makedirs('dataset_nii/train/labels', exist_ok=True)\n",
    "\n",
    "\n",
    "# get the name of images and labels\n",
    "files = df['image_id'].values\n",
    "# from these folders get the images and labels\n",
    "image_folder = 'New'\n",
    "label_folder = 'segmentations' \n",
    "import glob \n",
    "import tqdm\n",
    "for i in tqdm.tqdm(range(len(files))):\n",
    "\n",
    "    images = 'New/'+files[i]+'/ct.nii.gz'\n",
    "    labels = 'New/'+files[i]+'/segmentations/combined_mask.nii.gz'\n",
    "    # copy the images, change the name of the images sand then save them in the train folder\n",
    "    import shutil\n",
    "    shutil.copy(images, 'dataset_nii/train/images/'+files[i]+'.nii.gz') \n",
    "    shutil.copy(labels, 'dataset_nii/train/labels/'+files[i]+'.nii.gz')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['image_id', 'age', 'gender', 'institute', 'study_type', 'split',\n",
      "       'manufacturer', 'scanner_model', 'kvp', 'pathology',\n",
      "       'pathology_location'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [00:01<00:00, 63.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# read the mketa.xlsx file \n",
    "import pandas as pd\n",
    "df = pd.read_excel('meta.xlsx')\n",
    "# select only train values in split column\n",
    "df = df[df['split']=='test']\n",
    "print(df.columns)\n",
    "# make directories for train and test\n",
    "import os\n",
    "\n",
    "os.makedirs('dataset_nii/test/images', exist_ok=True)\n",
    "os.makedirs('dataset_nii/test/labels', exist_ok=True)\n",
    "\n",
    "# get the name of images and labels\n",
    "files = df['image_id'].values\n",
    "# from these folders get the images and labels\n",
    "image_folder = 'New'\n",
    "label_folder = 'segmentations' \n",
    "import glob \n",
    "import tqdm\n",
    "for i in tqdm.tqdm(range(len(files))):\n",
    "\n",
    "    images = 'New/'+files[i]+'/ct.nii.gz'\n",
    "    labels = 'New/'+files[i]+'/segmentations/combined_mask.nii.gz'\n",
    "    # copy the images, change the name of the images sand then save them in the train folder\n",
    "    import shutil\n",
    "    shutil.copy(images, 'dataset_nii/test/images/'+files[i]+'.nii.gz') \n",
    "    shutil.copy(labels, 'dataset_nii/test/labels/'+files[i]+'.nii.gz')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['image_id', 'age', 'gender', 'institute', 'study_type', 'split',\n",
      "       'manufacturer', 'scanner_model', 'kvp', 'pathology',\n",
      "       'pathology_location'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 79.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# read the mketa.xlsx file \n",
    "import pandas as pd\n",
    "df = pd.read_excel('meta.xlsx')\n",
    "# select only train values in split column\n",
    "df = df[df['split']=='val']\n",
    "print(df.columns)\n",
    "# make directories for train and test\n",
    "import os\n",
    "\n",
    "os.makedirs('dataset_nii/val/images', exist_ok=True)\n",
    "os.makedirs('dataset_nii/val/labels', exist_ok=True)\n",
    "\n",
    "# get the name of images and labels\n",
    "files = df['image_id'].values\n",
    "# from these folders get the images and labels\n",
    "image_folder = 'New'\n",
    "label_folder = 'segmentations' \n",
    "import glob \n",
    "import tqdm\n",
    "for i in tqdm.tqdm(range(len(files))):\n",
    "\n",
    "    images = 'New/'+files[i]+'/ct.nii.gz'\n",
    "    labels = 'New/'+files[i]+'/segmentations/combined_mask.nii.gz'\n",
    "    # copy the images, change the name of the images sand then save them in the train folder\n",
    "    import shutil\n",
    "    shutil.copy(images, 'dataset_nii/val/images/'+files[i]+'.nii.gz') \n",
    "    shutil.copy(labels, 'dataset_nii/val/labels/'+files[i]+'.nii.gz')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True\n"
     ]
    }
   ],
   "source": [
    "print(len('datase_nii/train/images/') == len('datase_nii/train/labels/'), len('datase_nii/test/images/') == len('datase_nii/test/labels/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "def convert_mask_to_segmentation_(mask_path, output_txt_path, class_mapping):\n",
    "    # Load the mask image\n",
    "    mask_image = Image.open(mask_path)\n",
    "    \n",
    "    # Ensure the mask is in integer mode\n",
    "    if mask_image.mode not in ['L', 'I']:\n",
    "        mask_image = mask_image.convert('I')\n",
    "    \n",
    "    # Convert the mask image to a numpy array\n",
    "    mask_np = np.array(mask_image).astype(np.int32)\n",
    "    \n",
    "    # Identify unique labels (exclude 0 if it's background)\n",
    "    unique_labels = np.unique(mask_np)\n",
    "    unique_labels = unique_labels[unique_labels != 0]\n",
    "    \n",
    "    # Image dimensions for normalization\n",
    "    height, width = mask_np.shape[:2]\n",
    "    \n",
    "    # Open the output file\n",
    "    with open(output_txt_path, 'w') as f:\n",
    "        for label in unique_labels:\n",
    "            # Verify the label exists in class_mapping\n",
    "            if label not in class_mapping:\n",
    "                print(f\"Warning: Label {label} not found in class mapping.\")\n",
    "                continue\n",
    "            \n",
    "            # Create a binary mask for the current label\n",
    "            mask_bin = (mask_np == label).astype(np.uint8) * 255  # Ensure binary mask with 0 and 255\n",
    "            \n",
    "            # Determine OpenCV version for findContours compatibility\n",
    "            if int(cv2.__version__.split('.')[0]) >= 4:\n",
    "                contours, _ = cv2.findContours(mask_bin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            else:\n",
    "                _, contours, _ = cv2.findContours(mask_bin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            for contour in contours:\n",
    "                if len(contour) < 3:\n",
    "                    continue  # Skip if contour has less than 3 points\n",
    "\n",
    "                # Reshape and normalize the contour coordinates\n",
    "                contour = contour.reshape(-1, 2).astype(float)\n",
    "                contour[:, 0] = contour[:, 0] / width  # Normalize x\n",
    "                contour[:, 1] = contour[:, 1] / height  # Normalize y\n",
    "                segmentation = contour.flatten().round(6).tolist()\n",
    "                \n",
    "                # Format segmentation points as strings\n",
    "                segmentation_str = ' '.join(f\"{coord:.6f}\" for coord in segmentation)\n",
    "                class_id = class_mapping[label]\n",
    "                \n",
    "                # Write to file in format: <class_id> <points>\n",
    "                f.write(f\"{class_id} {segmentation_str}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_mapping = {0: 1, 1: 2, 2: 3, 3: 4, 4:5, 5:6, 6:7, 7:8, 8:9, 9:10, 10:11, 11:12, 12:13, 13:14, 14:15, 15:16, 16:17, 17:18, 18:19, 19:20, 20:21, 21:22, 22:23, 23:24, \n",
    "                 24:25, 25:26, 26:27, 27:28, 28:29, 29:30, 30:31, 31:32, 32:33, 33:34, 34:35, 35:36, 36:37, 37:38, 38:39, 39:40, 40:41, 41:42, 42:43, 43:44, 44:45, 45:46, \n",
    "                 46:47, 47:48, 48:49, 49:50, 50:51, 51:52, 52:53, 53:54, 54:55, 55:56, 56:57, 57:58, 58:59, 59:60, 60:61, 61:62, 62:63, 63:64, 64:65, 65:66, 66:67, 67:68, \n",
    "                 68:69, 69:70, 70:71, 71:72, 72:73, 73:74, 74:75, 75:76, 76:77, 77:78, 78:79, 79:80, 80:81, 81:82, 82:83, 83:84, 84:85, 85:86, 86:87, 87:88, 88:89, 89:90, 90:91,\n",
    "                   91:92, 92:93, 93:94, 94:95, 95:96, 96:97, 97:98, 98:99, 99:100, 100:101, 101:102, 102:103, 103:104, 104:105, 105:106, 106:107, 107:108, 108:109, 109:110, 110:111, \n",
    "                   111:112, 112:113, 113:114, 114:115, 115:116, 116:117, 117:118, 118:119, 119:120, 120:121, 121:122, 122:123, 123:124, 124:125, 125:126, 126:127, 127:128, 128:129, \n",
    "                   129:130, 130:131, 131:132, 132:133, 133:134, 134:135, 135:136, 136:137, 137:138, 138:139, 139:140, 140:141, 141:142, 142:143, 143:144, 144:145, 145:146, 146:147, \n",
    "                   147:148, 148:149, 149:150, 150:151, 151:152, 152:153, 153:154, 154:155, 155:156, 156:157, 157:158, 158:159, 159:160, 160:161, 161:162, 162:163, 163:164, 164:165, \n",
    "                   165:166, 166:167, 167:168, 168:169, 169:170, 170:171, 171:172, 172:173, 173:174, 174:175, 175:176, 176:177, 177:178, 178:179, 179:180, 180:181, 181:182, 182:183,\n",
    "                     183:184, 184:185, 185:186, 186:187, 187:188, 188:189, 189:190, 190:191, 191:192, 192:193, 193:194, 194:195, 195:196, 196:197, 197:198, 198:199, 199:200, 200:201}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset/train/labels/s0647.nii.gz_slice_227_0_.txt'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob \n",
    "file = glob.glob('dataset/train/labels/*')\n",
    "file[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_path = 'dataset/train/labels/s0001.nii.gz'\n",
    "convert_mask_to_segmentation_(file[0], 'output1_txt_path.txt', class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 83 fields in line 5, saw 157\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# check text file. \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput1_txt_path.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/med_yolo/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/med_yolo/lib/python3.11/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/med_yolo/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/med_yolo/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 83 fields in line 5, saw 157\n"
     ]
    }
   ],
   "source": [
    "# check text file. \n",
    "import pandas as pd\n",
    "df = pd.read_csv('output1_txt_path.txt', header=None, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>6 0.606557 0.148649 0.598361 0.162162 0.590164 0.162162 0.573770 0.189189 0.573770 0.202703 0.557377 0.229730 0.557377 0.243243 0.540984 0.270270 0.540984 0.283784 0.532787 0.297297 0.532787 0.324324 0.524590 0.337838 0.532787 0.351351 0.532787 0.378378 0.540984 0.391892 0.540984 0.405405 0.557377 0.432432 0.565574 0.432432 0.573770 0.445946 0.590164 0.445946 0.598361 0.459459 0.655738 0.459459 0.663934 0.445946 0.680328 0.445946 0.688525 0.432432 0.696721 0.432432 0.704918 0.418919 0.713115 0.418919 0.721311 0.405405 0.729508 0.405405 0.754098 0.364865 0.754098 0.283784 0.762295 0.270270 0.762295 0.229730 0.737705 0.189189 0.721311 0.189189 0.713115 0.175676 0.688525 0.175676 0.680328 0.162162 0.631148 0.162162 0.622951 0.148649</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7 0.614754 0.621622 0.606557 0.635135 0.590164...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84 0.803279 0.432432 0.778689 0.472973 0.77868...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98 0.926230 0.364865 0.918033 0.378378 0.91803...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99 0.803279 0.243243 0.778689 0.283784 0.77868...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  6 0.606557 0.148649 0.598361 0.162162 0.590164 0.162162 0.573770 0.189189 0.573770 0.202703 0.557377 0.229730 0.557377 0.243243 0.540984 0.270270 0.540984 0.283784 0.532787 0.297297 0.532787 0.324324 0.524590 0.337838 0.532787 0.351351 0.532787 0.378378 0.540984 0.391892 0.540984 0.405405 0.557377 0.432432 0.565574 0.432432 0.573770 0.445946 0.590164 0.445946 0.598361 0.459459 0.655738 0.459459 0.663934 0.445946 0.680328 0.445946 0.688525 0.432432 0.696721 0.432432 0.704918 0.418919 0.713115 0.418919 0.721311 0.405405 0.729508 0.405405 0.754098 0.364865 0.754098 0.283784 0.762295 0.270270 0.762295 0.229730 0.737705 0.189189 0.721311 0.189189 0.713115 0.175676 0.688525 0.175676 0.680328 0.162162 0.631148 0.162162 0.622951 0.148649\n",
       "0  7 0.614754 0.621622 0.606557 0.635135 0.590164...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "1  84 0.803279 0.432432 0.778689 0.472973 0.77868...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "2  98 0.926230 0.364865 0.918033 0.378378 0.91803...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "3  99 0.803279 0.243243 0.778689 0.283784 0.77868...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
